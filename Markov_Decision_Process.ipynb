{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPbGAYE8yZK/pd0cSsOpII",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIVAGORAM/DEEPLEARNING/blob/main/Markov_Decision_Process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%9921005015\n",
        "#MARKOV DECISION PROCESS\n",
        "import numpy as np\n",
        "\n",
        "class SimpleMDP:\n",
        "    def __init__(self):\n",
        "        self.num_states = 3\n",
        "        self.num_actions = 2\n",
        "        self.transitions = np.zeros((self.num_states, self.num_actions, self.num_states)) # transition probabilities\n",
        "        self.rewards = np.zeros((self.num_states, self.num_actions)) # rewards\n",
        "\n",
        "        # Define transition probabilities and rewards\n",
        "        self.transitions[0, 0, :] = [0.7, 0.3, 0.0]\n",
        "        self.transitions[0, 1, :] = [0.0, 0.9, 0.1]\n",
        "        self.transitions[1, 0, :] = [0.0, 1.0, 0.0]\n",
        "        self.transitions[1, 1, :] = [0.8, 0.2, 0.0]\n",
        "        self.transitions[2, :, :] = [0.0, 0.0, 1.0]\n",
        "        self.rewards[1, 0] = 1.0\n",
        "        self.rewards[1, 1] = 1.0\n",
        "        self.rewards[2, :] = 10.0\n",
        "\n",
        "    def step(self, state, action):\n",
        "        next_state_probs = self.transitions[state, action, :]\n",
        "        next_state = np.random.choice(self.num_states, p=next_state_probs)\n",
        "        reward = self.rewards[state, action]\n",
        "        return next_state, reward\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state, :])\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        best_next_action = np.argmax(self.q_table[next_state, :])\n",
        "        td_target = reward + self.discount_factor * self.q_table[next_state, best_next_action]\n",
        "        td_error = td_target - self.q_table[state, action]\n",
        "        self.q_table[state, action] += self.learning_rate * td_error\n",
        "\n",
        "# Create MDP\n",
        "mdp = SimpleMDP()\n",
        "\n",
        "# Create Q-learning agent\n",
        "agent = QLearningAgent(num_states=mdp.num_states, num_actions=mdp.num_actions)\n",
        "\n",
        "# Training\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = np.random.randint(mdp.num_states) # Start from a random state\n",
        "    while state != 2: # Continue until reaching the terminal state\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward = mdp.step(state, action)\n",
        "        agent.update_q_table(state, action, reward, next_state)\n",
        "        state = next_state\n",
        "\n",
        "# Evaluate\n",
        "total_rewards = 0\n",
        "num_eval_episodes = 100\n",
        "for episode in range(num_eval_episodes):\n",
        "    state = 0\n",
        "    while state != 2:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward = mdp.step(state, action)\n",
        "        total_rewards += reward\n",
        "        state = next_state\n",
        "\n",
        "average_reward = total_rewards / num_eval_episodes\n",
        "print(\"Average reward over\", num_eval_episodes, \"evaluation episodes:\", average_reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv8JUsW3Pfyn",
        "outputId": "85d28ea4-aca4-4263-b968-80b1cb43ae70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward over 100 evaluation episodes: 221.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LkDCT7WsXXYY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}